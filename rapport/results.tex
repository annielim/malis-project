\subsection{Evaluation method}
We evaluated our method with two different datasets : CREMI and ISBI datasets.\\
Both are composed of Drosophila melanogaster adult brain images.\\

A very simple architecture is used for the training.
It's composed of 6 layers of convolution.\\
In~\ref{turaga_maximin_2009}, the original architecture had 4 layers with 5 features. We decided to add more layers and features to obtain better results. 
Batch normalisations were also added as it didn't exist before.\\

\begin{center}
	\begin{tabular}{rllllll}\toprule
		Layer & Kernel & Strides & Features & BN &  Activation & Output shape \\
		\midrule
		Input  &  &  & & & & (21, 21, 1)  \\
		Convolution & (5, 5) & (1, 1) & 8 & Y & ReLU  & (21, 21, 8)  \\
		Convolution & (5, 5) & (1, 1) & 32 & Y & ReLU  & (21, 21, 32)  \\
		Convolution & (5, 5) & (1, 1) & 32 & Y & ReLU  & (21, 21, 32)  \\
		Convolution & (5, 5) & (1, 1) & 32 & Y & ReLU  & (21, 21, 32)  \\
		Convolution & (5, 5) & (1, 1) & 8 & Y & ReLU  & (21, 21, 8)  \\
		Convolution & (5, 5) & (1, 1) & 2 & N & sigmoid  & (21, 21, 2)  \\
		\bottomrule
	\end{tabular}
\end{center}

\subsection{CREMI}
The CREMI (Circuit Reconstruction from Electron Microscopy Images) dataset has three volumes but we decided to use only one (volume A) for our training.\\
This 3D image has a size of 1250x1250x125. Its corresponding grountruth was also provided in the dataset. 
The segmentation has labeled connected components with really thin edges.
For the evaluation, we used the CREMI library that was given with the dataset in Python 2. We, then, adapted it in Python 3.\\

Our 3D image of size X x Y x Z was easier to predict in two-dimension. 
That's why we considered Z images of size X x Y which were stacked together to get back a 3D image.\\


A question arises : how to get an image segmentation from the affinity graph ?\\

We used to different methods to answer this problem.\\
First of all, a BPT (Binary Partition Tree) and a graph cut could get a good image segmentation. 
However, even with a strong threshold (around 0.99), isthmus appeared and fusion two different objects together.
It's a big issue as it affects our scores.\\
Secondly, to get rid of isthmus, we did an average affinity and it could segmented the objects nicely.\\
Isthmus issues can be solved by improving our post-processing, and it should disappear with a better architecture.\\

Our results are promising as the different objects are well segmented. 
Yet, there are a few oversegmentations when regions are darker.\\ 
Nucleus are not detected as a same object as the cell.\\

The second volume (volume B) was used as the test set.
The image was similar to the one in the first volume but the objects are more stretched out.\\
Here the objects are well segmented but it was harder on darker regions.\\

With more details, we can evaluate with numerical results, according to the Rand index and the VOI (variation of information) merge and split. 
"The Rand index is a measure of the similarity between two data clustering."
*explain VOI*\\
The Rand index should be the highest as possible, closer to 1. 
The metric VOI should be lower to be better.\\

In the original architecture of 4 layers, the Rand index is 0.53 while we got 0.61 with our training set and 0.53 with our test set.
Our Rand index is higher to the one of the original architecture.
Moreover our VOI is low so it's corresponding to our desired outcome.\\

Thus, our results are hopeful knowing our architecture used was really simple.\\ 
It's still far from the state of the art but it could get even better with a more complex network.\\

\subsection{ISBI 2012}
In a second part, we evaluated our method on the ISBI 2012 Challenge dataset which is a set of 30 sections from a serial section Transmission Electron Microscopy (ssTEM) data set of Drosophila first instar larva ventral nerve cord(VNC).
The dataset also provides a corresponding groundtruth with labeled connected components, and thick contours.
There is no test set available because we should submit our method to the challenge's leaderboard to get our scores.
The image has a size of 512x512x30 which is smaller than the CREMI's.
We evaluated using FIJI (Fiji Is Just Imagej) as required in the challenge.

The exact same architecture than CREMI's was used for the ISBI dataset.

We still get good results even with a simple architecture, because the different objects are well seperated.
However, there are oversegmentations for darker regions as well.
We made the hypothesis that the oversegmentation issue come from the lack of contrast of the image.

To get numerical results we had to submit our method to the ISBI challenge.
We evaluate through the same metrics as for the CREMI dataset, that is to say the Rand index and the VOI.
This time the VOI should be higher to be better.
The Rand index of our training set was 0.76 and 0.73 for the test test, which is similar but still higher than the thresholding with a Rand index of 0.72.
Our VOI is also a bit higher than the threshold. 
It means we could have very promising results with our actual method.
We could get even closer to the state of the art if we adopt an improved method.


