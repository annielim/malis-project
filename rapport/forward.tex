\subsection{Improvement on MALIS}

As we have seen before, MALIS performs really well, but can still be
improved.\\
It was most notably improved in~\cite{funke_large_2019} where they were able to
improve the affinity prediction using more recent architectures, by improving
the training and taking full advantage of the MST and by applying a
post-processing on the affinity graph instead of a simple thresholding.

\begin{figure}[!htbp]
	\centering
	\includegraphics[width=0.8\linewidth]{./images/mala_process.png}
	\caption{Improves MALIS as described in~\cite{funke_large_2019}}%
	\label{fig:mala_process}
\end{figure}

As we can see in figure~\ref{fig:mala_process} the CNN was replaced by a U-Net,
which we will describe afterwards. Then the segmentation is obtained using a
seeded-watershed, which is then improved using a percentile agglomeration of
small objects.

\subsubsection{Using a more potent architecture}
Indeed, one of the limits of the previous method was the use of a relatively
simple neural network to predict the affinity. This is mostly due to the fact
that neural networks have greatly improved since the original
paper~\cite{turaga_maximin_2009} in 2009.\\

\begin{figure}[!htbp]
	\centering
	\includegraphics[width=0.8\linewidth]{./images/mala_architecture.png}
	\caption{U-Net architecture used on the CREMI dataset from~\cite{funke_large_2019}}%
	\label{fig:mala_unet}
\end{figure}


U-nets especially have been widely used for image segmentation and are thus a
natural choice in our case. The architecture used in~\cite{funke_large_2019}
is shown in figure~\ref{fig:mala_unet}. As we can see, the training will
obviously be much longer than previously, but the results should be
significantly better.\\
A question that can arise is why not simply use just a U-Net without the MALIS
loss, and as we will see later, the MALIS loss gives us better results on
differents measures of segmentation quality.


\subsubsection{Constrained MALIS loss}

Previously, we only computed the maximin edge for a pair of pixels (or a finite
amount of pairs), which means that some information from the MST was not
used.\\
However, we would like to compute the maximin edge for all pairs of pixels in
the image. This was not done in the previous paper for time efficiency
reasons.\\
However they describe a way to compute the loss with in quasilinear time instead
of in polynomial time.\\

The main idea is that all maximin edges are in the MST, and there are $n-1$
edges if we have $n$ pixels in our image. We could then simply compute the loss
over all those edges but this would mean that they are all as important as the
others. However since we have $n^2$ pairs of points and $n-1$ edges in our MST,
they will be the maximin edge for a different number of pixel-pairs. So we must
find a way to see how often an edge from the MST is a maximin edge.\\

We won't go into much details here on how exactly this is done, but they are
able to find for how many pairs of pixels an edge is a maximin edge when
constructing the MST.\\
When we add an edge using Kruskal's algorithm, we can look at the "size" of the
trees it merges and deduce the number of pairs for which the current edge is
the maximin edge.\\

From this, we can define the positive weight of an edge $e$ as the number of pairs
from the same object/segment merged by adding $e$ to the MST. More formally we
have :
\begin{equation*}
	w_p(e)=\lvert \{(u,v)\in F^2 \;|\;\delta(u,v)=1, e=mm(u,v) \}   \rvert
\end{equation*}

Similarly we can define the negative weight of an edge as :
\begin{equation*}
	w_n(e)=\lvert \{(u,v)\in F^2 \;|\;\delta(u,v)=0, e=mm(u,v) \}   \rvert
\end{equation*}

These formulations allow us to rewrite our loss function as :

\begin{equation*}
	L(I,\theta,S) = \sum_{e\in MST(G)} w_p(e)l(1,A_e(I,\theta)) + w_nl(0,A_e(I,\theta))
\end{equation*}
With $A_e$ the affinity of an edge $e$.\\

With this loss function being able to be computed in quasilinear time, this
will allow us to use the whole patch for the loss computation instead of a few
pairs of pixels, which should improve the results. This loss function is the
same as before, it still is related to the Rand Index, but it should now
approximate it much better.\\


%Vraiment pas convaincu de cette partie
However this doesn't really solve the issue of class imbalance that we noted
before. They suggest a way to resolve this by first computing the positive part
of the loss first (with $w_p$) and then the negative part (with $w_n$). In both
case, we set the other edges (the edges between objects for the positive pass)
to their groundtruth value so that we only take into account a category of
edges.\\
This means that we will computethe loss for edges between objects and edges
inside objects separately, which allows use to combine them afterwards with the
appropriate weights so which should resolve the class imbalance issue.

\subsubsection{Seeded watershed as post processing}

Remember that before, we computed the segmentation by thresholding our affinity
graph. However this doesn't give optimal results, as for example locally
another threshold would perform better.\\
An issue that was also encountered was small objects that were inside bigger
ones. These objects lead to an oversegmentation and we would like a way to
automatically remove those inaccuracies, or at least part of them.

\begin{figure}[!htbp]
	\centering
	\includegraphics[width=0.5\linewidth]{./images/mala_post_proc.png}
	\caption{Seeded watershed on an affinity graph, as described in~\cite{funke_large_2019}}%
	\label{fig:seeded_ws}
\end{figure}

This is where the framework described in~\cite{funke_large_2019} comes in play.
As we can see in figure~\ref{fig:seeded_ws}, the first step of the process is
to average the affinities, as we did before. Afterwards the averaged affinities
are thresholded at 0.5 (here this threshold is not a parameter). Then a
distance transform (or a distance map) is computed from the objects to the
borders. In this distance transform, the furthest points from the borders will
have the higher values. Then, all the local maxima are taken as seeds for a
seeded watershed, which gives us a first segmentation.\\

This is still an oversegmentation, but a fragment agglomeration algorithm is
then used to fuse regions together. multiple criteria can be used to
determine which regions to merge together and they are described
in~\cite{funke_large_2019}.\\
When all of those steps are done, we obtain the final segmentation.\\

As we can see the process is greatly improved from the first version of MALIS,
with the loss function, the neural network architecture and the post processign
used being much more powerful than their previous counterparts.

\subsection{Goals for the second semester}

The main goal for this next semester will be to implement this improved MALIS.
The mains components to implement are as follow:
\begin{itemize}
	\item Implement the U-Net
	\item Implement the constrained MALIS loss, with it's optimizations
	\item Implement the seeded watershed and fragment agglomeration
	\item Try other methods of post processing
\end{itemize}

Although this is a very important step and a large amount of work, the good
thing with theses improvements is that we can still use parts of the older
method while implementing new ones. For example we can try the U-Net with the
first MALIS loss and no post processing, and the same is true for every new
component that we will implement.\\

Once we have a working implementation the goal will be to try the method on
various datasets. The CREMI dataset will provide us a baseline to compare our
results with those obtained in~\cite{funke_large_2019}. We will also try the
method on other dataset such as the BSDS500 dataset, which is composed of more
varied types of images and not just connectomes.\\

These are our main goals for the second semester, but if we are able to finish
all of this early, there are a lot of interesting experiments where we could
further test the limits of this method.
